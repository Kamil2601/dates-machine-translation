{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import load_dataset, SpecialTokens\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from models import EncoderGRU, DecoderGRU, Seq2sec\n",
    "from train import Trainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 34490.33it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 34343.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_size = 10000\n",
    "val_size = 1000\n",
    "train_data, human_vocab, machine_vocab = load_dataset(train_size)\n",
    "val_data, _, _ = load_dataset(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{';': 0,\n",
       " '?': 1,\n",
       " ' ': 2,\n",
       " '.': 3,\n",
       " '/': 4,\n",
       " '0': 5,\n",
       " '1': 6,\n",
       " '2': 7,\n",
       " '3': 8,\n",
       " '4': 9,\n",
       " '5': 10,\n",
       " '6': 11,\n",
       " '7': 12,\n",
       " '8': 13,\n",
       " '9': 14,\n",
       " 'a': 15,\n",
       " 'b': 16,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 19,\n",
       " 'f': 20,\n",
       " 'g': 21,\n",
       " 'h': 22,\n",
       " 'i': 23,\n",
       " 'j': 24,\n",
       " 'l': 25,\n",
       " 'm': 26,\n",
       " 'n': 27,\n",
       " 'o': 28,\n",
       " 'p': 29,\n",
       " 'r': 30,\n",
       " 's': 31,\n",
       " 't': 32,\n",
       " 'u': 33,\n",
       " 'v': 34,\n",
       " 'w': 35,\n",
       " 'y': 36}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{';': 0,\n",
       " '>': 1,\n",
       " '<': 2,\n",
       " '-': 3,\n",
       " '0': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '5': 9,\n",
       " '6': 10,\n",
       " '7': 11,\n",
       " '8': 12,\n",
       " '9': 13}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8/8/71', '>1971-08-08<'),\n",
       " ('3/3/81', '>1981-03-03<'),\n",
       " ('4/8/96', '>1996-04-08<'),\n",
       " ('5/5/97', '>1997-05-05<'),\n",
       " ('9/7/98', '>1998-09-07<'),\n",
       " ('4/4/99', '>1999-04-04<'),\n",
       " ('2/8/02', '>2002-02-08<'),\n",
       " ('9/1/13', '>2013-09-01<'),\n",
       " ('6/2/75', '>1975-06-02<'),\n",
       " ('1/1/10', '>2010-01-01<'),\n",
       " ('1/8/21', '>2021-01-08<'),\n",
       " ('1/6/72', '>1972-01-06<'),\n",
       " ('8/8/92', '>1992-08-08<'),\n",
       " ('4/2/05', '>2005-04-02<'),\n",
       " ('5/4/71', '>1971-05-04<'),\n",
       " ('4/8/96', '>1996-04-08<'),\n",
       " ('6/2/23', '>2023-06-02<'),\n",
       " ('4/6/86', '>1986-04-06<'),\n",
       " ('9/1/98', '>1998-09-01<'),\n",
       " ('6/6/92', '>1992-06-06<')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8/4/21', '>2021-08-04<'),\n",
       " ('1/2/80', '>1980-01-02<'),\n",
       " ('2/1/88', '>1988-02-01<'),\n",
       " ('3/5/76', '>1976-03-05<'),\n",
       " ('6/4/90', '>1990-06-04<'),\n",
       " ('1/2/74', '>1974-01-02<'),\n",
       " ('9/27/85', '>1985-09-27<'),\n",
       " ('1 11 89', '>1989-11-01<'),\n",
       " ('8 06 88', '>1988-06-08<'),\n",
       " ('6/10/23', '>2023-06-10<'),\n",
       " ('9/30/06', '>2006-09-30<'),\n",
       " ('2/14/00', '>2000-02-14<'),\n",
       " ('1/11/12', '>2012-01-11<'),\n",
       " ('6 09 16', '>2016-09-06<'),\n",
       " ('8 05 16', '>2016-05-08<'),\n",
       " ('6/28/78', '>1978-06-28<'),\n",
       " ('1 08 84', '>1984-08-01<'),\n",
       " ('6/17/06', '>2006-06-17<'),\n",
       " ('5 07 09', '>2009-07-05<'),\n",
       " ('7/26/79', '>1979-07-26<')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def _get_char(self, ind):\n",
    "        if isinstance(ind, torch.Tensor):\n",
    "            return self.inv_vocab[ind.item()]\n",
    "        else:\n",
    "            return self.inv_vocab[ind]\n",
    "\n",
    "    def __init__(self, vocab: dict):\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = {v:k for k,v in vocab.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "    def str_to_ind(self, str):\n",
    "        return [self.vocab[c] for c in str]\n",
    "    \n",
    "    def ind_to_str(self, ind):\n",
    "        return ''.join([self._get_char(i) for i in ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8/71\n",
      "[13, 4, 13, 4, 12, 6]\n",
      "8/8/71\n"
     ]
    }
   ],
   "source": [
    "test = Lang(human_vocab)\n",
    "date = train_data[0][0]\n",
    "print(date)\n",
    "translated_date = test.str_to_ind(date)\n",
    "print(translated_date)\n",
    "reversed_translation = test.ind_to_str(translated_date)\n",
    "print(reversed_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationTrainingDataset(Dataset):\n",
    "    def __init__(self, data, input_vocab, output_vocab):\n",
    "        self.input_lang = Lang(input_vocab)\n",
    "        self.target_lang = Lang(output_vocab)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.encoder_inputs = [self.input_lang.str_to_ind(input_sent) for input_sent, _ in self.data]\n",
    "\n",
    "        targets = [self.target_lang.str_to_ind(target_sent) for _, target_sent in self.data]\n",
    "        self.decoder_inputs = [target[:-1] for target in targets]\n",
    "        self.decoder_targets = [target[1:] for target in targets]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoder_inputs[index], self.decoder_inputs[index], self.decoder_targets[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationTrainingDataset(train_data, human_vocab, machine_vocab)\n",
    "val_dataset = TranslationTrainingDataset(val_data, human_vocab, machine_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 4, 13, 4, 12, 6] 8/8/71\n",
      "[1, 5, 13, 11, 5, 3, 4, 12, 3, 4, 12] >1971-08-08\n",
      "[5, 13, 11, 5, 3, 4, 12, 3, 4, 12, 2] 1971-08-08<\n"
     ]
    }
   ],
   "source": [
    "x,y,z = train_dataset[0]\n",
    "print(x, train_dataset.input_lang.ind_to_str(x))\n",
    "print(y, train_dataset.target_lang.ind_to_str(y))\n",
    "print(z, train_dataset.target_lang.ind_to_str(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(data):\n",
    "    batch = []\n",
    "    for i in range(len(data[0])):\n",
    "        batch_data = [torch.tensor(item[i], dtype=torch.int64) for item in data]\n",
    "        batch_data = nn.utils.rnn.pad_sequence(batch_data, batch_first=True)\n",
    "        batch.append(batch_data)\n",
    "\n",
    "\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, collate_fn=collate_batch, batch_size = 64, num_workers = 8)\n",
    "val_loader = DataLoader(dataset=val_dataset, collate_fn=collate_batch, batch_size = 64, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13,  4, 13,  4, 12,  6],\n",
      "        [ 8,  4,  8,  4, 13,  6],\n",
      "        [ 9,  4, 13,  4, 14, 11],\n",
      "        [10,  4, 10,  4, 14, 12],\n",
      "        [14,  4, 12,  4, 14, 13],\n",
      "        [ 9,  4,  9,  4, 14, 14],\n",
      "        [ 7,  4, 13,  4,  5,  7],\n",
      "        [14,  4,  6,  4,  6,  8],\n",
      "        [11,  4,  7,  4, 12, 10],\n",
      "        [ 6,  4,  6,  4,  6,  5],\n",
      "        [ 6,  4, 13,  4,  7,  6],\n",
      "        [ 6,  4, 11,  4, 12,  7],\n",
      "        [13,  4, 13,  4, 14,  7],\n",
      "        [ 9,  4,  7,  4,  5, 10],\n",
      "        [10,  4,  9,  4, 12,  6],\n",
      "        [ 9,  4, 13,  4, 14, 11],\n",
      "        [11,  4,  7,  4,  7,  8],\n",
      "        [ 9,  4, 11,  4, 13, 11],\n",
      "        [14,  4,  6,  4, 14, 13],\n",
      "        [11,  4, 11,  4, 14,  7],\n",
      "        [ 8,  4,  6,  4, 13, 11],\n",
      "        [ 9,  4,  6,  4,  5, 11],\n",
      "        [14,  4,  9,  4, 12,  8],\n",
      "        [ 7,  4, 11,  4,  7,  7],\n",
      "        [ 8,  4, 10,  4,  6,  7],\n",
      "        [11,  4,  6,  4,  6,  7],\n",
      "        [ 6,  4,  9,  4, 14, 13],\n",
      "        [14,  4,  8,  4,  6,  8],\n",
      "        [ 6,  4, 12,  4, 14, 13],\n",
      "        [ 7,  4, 13,  4, 12,  9],\n",
      "        [ 7,  4, 11,  4, 14,  8],\n",
      "        [13,  4,  9,  4, 14,  7],\n",
      "        [ 7,  4,  7,  4, 12,  7],\n",
      "        [ 9,  4,  8,  4, 13,  9],\n",
      "        [13,  4, 13,  4, 13, 14],\n",
      "        [11,  4,  6,  4, 12,  5],\n",
      "        [ 7,  4, 11,  4, 12, 12],\n",
      "        [12,  4,  9,  4, 13, 11],\n",
      "        [14,  4, 14,  4,  7,  5],\n",
      "        [11,  4, 14,  4, 14, 11],\n",
      "        [ 7,  4,  6,  4,  7,  7],\n",
      "        [13,  4, 13,  4,  5, 12],\n",
      "        [ 8,  4,  9,  4, 14,  8],\n",
      "        [14,  4, 14,  4, 12, 13],\n",
      "        [11,  4,  6,  4, 14, 14],\n",
      "        [ 6,  4,  9,  4,  7,  8],\n",
      "        [ 8,  4,  7,  4, 12,  6],\n",
      "        [12,  4, 12,  4, 12, 13],\n",
      "        [10,  4,  7,  4,  5,  7],\n",
      "        [12,  4,  7,  4,  7,  5],\n",
      "        [13,  4, 10,  4, 14,  6],\n",
      "        [ 6,  4, 12,  4,  6, 12],\n",
      "        [11,  4, 11,  4, 13,  6],\n",
      "        [14,  4,  7,  4, 13, 12],\n",
      "        [ 8,  4, 10,  4, 12, 13],\n",
      "        [14,  4, 13,  4,  5,  5],\n",
      "        [ 8,  4, 12,  4,  6, 12],\n",
      "        [12,  4,  9,  4, 13,  8],\n",
      "        [12,  4,  6,  4,  5, 14],\n",
      "        [12,  4, 14,  4, 14, 12],\n",
      "        [12,  4, 11,  4, 14,  8],\n",
      "        [13,  4, 14,  4, 13, 13],\n",
      "        [ 9,  4,  7,  4,  5, 12],\n",
      "        [11,  4,  6,  4,  6, 12]])\n",
      "tensor([[ 1,  5, 13, 11,  5,  3,  4, 12,  3,  4, 12],\n",
      "        [ 1,  5, 13, 12,  5,  3,  4,  7,  3,  4,  7],\n",
      "        [ 1,  5, 13, 13, 10,  3,  4,  8,  3,  4, 12],\n",
      "        [ 1,  5, 13, 13, 11,  3,  4,  9,  3,  4,  9],\n",
      "        [ 1,  5, 13, 13, 12,  3,  4, 13,  3,  4, 11],\n",
      "        [ 1,  5, 13, 13, 13,  3,  4,  8,  3,  4,  8],\n",
      "        [ 1,  6,  4,  4,  6,  3,  4,  6,  3,  4, 12],\n",
      "        [ 1,  6,  4,  5,  7,  3,  4, 13,  3,  4,  5],\n",
      "        [ 1,  5, 13, 11,  9,  3,  4, 10,  3,  4,  6],\n",
      "        [ 1,  6,  4,  5,  4,  3,  4,  5,  3,  4,  5],\n",
      "        [ 1,  6,  4,  6,  5,  3,  4,  5,  3,  4, 12],\n",
      "        [ 1,  5, 13, 11,  6,  3,  4,  5,  3,  4, 10],\n",
      "        [ 1,  5, 13, 13,  6,  3,  4, 12,  3,  4, 12],\n",
      "        [ 1,  6,  4,  4,  9,  3,  4,  8,  3,  4,  6],\n",
      "        [ 1,  5, 13, 11,  5,  3,  4,  9,  3,  4,  8],\n",
      "        [ 1,  5, 13, 13, 10,  3,  4,  8,  3,  4, 12],\n",
      "        [ 1,  6,  4,  6,  7,  3,  4, 10,  3,  4,  6],\n",
      "        [ 1,  5, 13, 12, 10,  3,  4,  8,  3,  4, 10],\n",
      "        [ 1,  5, 13, 13, 12,  3,  4, 13,  3,  4,  5],\n",
      "        [ 1,  5, 13, 13,  6,  3,  4, 10,  3,  4, 10],\n",
      "        [ 1,  5, 13, 12, 10,  3,  4,  7,  3,  4,  5],\n",
      "        [ 1,  6,  4,  4, 10,  3,  4,  8,  3,  4,  5],\n",
      "        [ 1,  5, 13, 11,  7,  3,  4, 13,  3,  4,  8],\n",
      "        [ 1,  6,  4,  6,  6,  3,  4,  6,  3,  4, 10],\n",
      "        [ 1,  6,  4,  5,  6,  3,  4,  7,  3,  4,  9],\n",
      "        [ 1,  6,  4,  5,  6,  3,  4, 10,  3,  4,  5],\n",
      "        [ 1,  5, 13, 13, 12,  3,  4,  5,  3,  4,  8],\n",
      "        [ 1,  6,  4,  5,  7,  3,  4, 13,  3,  4,  7],\n",
      "        [ 1,  5, 13, 13, 12,  3,  4,  5,  3,  4, 11],\n",
      "        [ 1,  5, 13, 11,  8,  3,  4,  6,  3,  4, 12],\n",
      "        [ 1,  5, 13, 13,  7,  3,  4,  6,  3,  4, 10],\n",
      "        [ 1,  5, 13, 13,  6,  3,  4, 12,  3,  4,  8],\n",
      "        [ 1,  5, 13, 11,  6,  3,  4,  6,  3,  4,  6],\n",
      "        [ 1,  5, 13, 12,  8,  3,  4,  8,  3,  4,  7],\n",
      "        [ 1,  5, 13, 12, 13,  3,  4, 12,  3,  4, 12],\n",
      "        [ 1,  5, 13, 11,  4,  3,  4, 10,  3,  4,  5],\n",
      "        [ 1,  5, 13, 11, 11,  3,  4,  6,  3,  4, 10],\n",
      "        [ 1,  5, 13, 12, 10,  3,  4, 11,  3,  4,  8],\n",
      "        [ 1,  6,  4,  6,  4,  3,  4, 13,  3,  4, 13],\n",
      "        [ 1,  5, 13, 13, 10,  3,  4, 10,  3,  4, 13],\n",
      "        [ 1,  6,  4,  6,  6,  3,  4,  6,  3,  4,  5],\n",
      "        [ 1,  6,  4,  4, 11,  3,  4, 12,  3,  4, 12],\n",
      "        [ 1,  5, 13, 13,  7,  3,  4,  7,  3,  4,  8],\n",
      "        [ 1,  5, 13, 11, 12,  3,  4, 13,  3,  4, 13],\n",
      "        [ 1,  5, 13, 13, 13,  3,  4, 10,  3,  4,  5],\n",
      "        [ 1,  6,  4,  6,  7,  3,  4,  5,  3,  4,  8],\n",
      "        [ 1,  5, 13, 11,  5,  3,  4,  7,  3,  4,  6],\n",
      "        [ 1,  5, 13, 11, 12,  3,  4, 11,  3,  4, 11],\n",
      "        [ 1,  6,  4,  4,  6,  3,  4,  9,  3,  4,  6],\n",
      "        [ 1,  6,  4,  6,  4,  3,  4, 11,  3,  4,  6],\n",
      "        [ 1,  5, 13, 13,  5,  3,  4, 12,  3,  4,  9],\n",
      "        [ 1,  6,  4,  5, 11,  3,  4,  5,  3,  4, 11],\n",
      "        [ 1,  5, 13, 12,  5,  3,  4, 10,  3,  4, 10],\n",
      "        [ 1,  5, 13, 12, 11,  3,  4, 13,  3,  4,  6],\n",
      "        [ 1,  5, 13, 11, 12,  3,  4,  7,  3,  4,  9],\n",
      "        [ 1,  6,  4,  4,  4,  3,  4, 13,  3,  4, 12],\n",
      "        [ 1,  6,  4,  5, 11,  3,  4,  7,  3,  4, 11],\n",
      "        [ 1,  5, 13, 12,  7,  3,  4, 11,  3,  4,  8],\n",
      "        [ 1,  6,  4,  4, 13,  3,  4, 11,  3,  4,  5],\n",
      "        [ 1,  5, 13, 13, 11,  3,  4, 11,  3,  4, 13],\n",
      "        [ 1,  5, 13, 13,  7,  3,  4, 11,  3,  4, 10],\n",
      "        [ 1,  5, 13, 12, 12,  3,  4, 12,  3,  4, 13],\n",
      "        [ 1,  6,  4,  4, 11,  3,  4,  8,  3,  4,  6],\n",
      "        [ 1,  6,  4,  5, 11,  3,  4, 10,  3,  4,  5]])\n",
      "tensor([[ 5, 13, 11,  5,  3,  4, 12,  3,  4, 12,  2],\n",
      "        [ 5, 13, 12,  5,  3,  4,  7,  3,  4,  7,  2],\n",
      "        [ 5, 13, 13, 10,  3,  4,  8,  3,  4, 12,  2],\n",
      "        [ 5, 13, 13, 11,  3,  4,  9,  3,  4,  9,  2],\n",
      "        [ 5, 13, 13, 12,  3,  4, 13,  3,  4, 11,  2],\n",
      "        [ 5, 13, 13, 13,  3,  4,  8,  3,  4,  8,  2],\n",
      "        [ 6,  4,  4,  6,  3,  4,  6,  3,  4, 12,  2],\n",
      "        [ 6,  4,  5,  7,  3,  4, 13,  3,  4,  5,  2],\n",
      "        [ 5, 13, 11,  9,  3,  4, 10,  3,  4,  6,  2],\n",
      "        [ 6,  4,  5,  4,  3,  4,  5,  3,  4,  5,  2],\n",
      "        [ 6,  4,  6,  5,  3,  4,  5,  3,  4, 12,  2],\n",
      "        [ 5, 13, 11,  6,  3,  4,  5,  3,  4, 10,  2],\n",
      "        [ 5, 13, 13,  6,  3,  4, 12,  3,  4, 12,  2],\n",
      "        [ 6,  4,  4,  9,  3,  4,  8,  3,  4,  6,  2],\n",
      "        [ 5, 13, 11,  5,  3,  4,  9,  3,  4,  8,  2],\n",
      "        [ 5, 13, 13, 10,  3,  4,  8,  3,  4, 12,  2],\n",
      "        [ 6,  4,  6,  7,  3,  4, 10,  3,  4,  6,  2],\n",
      "        [ 5, 13, 12, 10,  3,  4,  8,  3,  4, 10,  2],\n",
      "        [ 5, 13, 13, 12,  3,  4, 13,  3,  4,  5,  2],\n",
      "        [ 5, 13, 13,  6,  3,  4, 10,  3,  4, 10,  2],\n",
      "        [ 5, 13, 12, 10,  3,  4,  7,  3,  4,  5,  2],\n",
      "        [ 6,  4,  4, 10,  3,  4,  8,  3,  4,  5,  2],\n",
      "        [ 5, 13, 11,  7,  3,  4, 13,  3,  4,  8,  2],\n",
      "        [ 6,  4,  6,  6,  3,  4,  6,  3,  4, 10,  2],\n",
      "        [ 6,  4,  5,  6,  3,  4,  7,  3,  4,  9,  2],\n",
      "        [ 6,  4,  5,  6,  3,  4, 10,  3,  4,  5,  2],\n",
      "        [ 5, 13, 13, 12,  3,  4,  5,  3,  4,  8,  2],\n",
      "        [ 6,  4,  5,  7,  3,  4, 13,  3,  4,  7,  2],\n",
      "        [ 5, 13, 13, 12,  3,  4,  5,  3,  4, 11,  2],\n",
      "        [ 5, 13, 11,  8,  3,  4,  6,  3,  4, 12,  2],\n",
      "        [ 5, 13, 13,  7,  3,  4,  6,  3,  4, 10,  2],\n",
      "        [ 5, 13, 13,  6,  3,  4, 12,  3,  4,  8,  2],\n",
      "        [ 5, 13, 11,  6,  3,  4,  6,  3,  4,  6,  2],\n",
      "        [ 5, 13, 12,  8,  3,  4,  8,  3,  4,  7,  2],\n",
      "        [ 5, 13, 12, 13,  3,  4, 12,  3,  4, 12,  2],\n",
      "        [ 5, 13, 11,  4,  3,  4, 10,  3,  4,  5,  2],\n",
      "        [ 5, 13, 11, 11,  3,  4,  6,  3,  4, 10,  2],\n",
      "        [ 5, 13, 12, 10,  3,  4, 11,  3,  4,  8,  2],\n",
      "        [ 6,  4,  6,  4,  3,  4, 13,  3,  4, 13,  2],\n",
      "        [ 5, 13, 13, 10,  3,  4, 10,  3,  4, 13,  2],\n",
      "        [ 6,  4,  6,  6,  3,  4,  6,  3,  4,  5,  2],\n",
      "        [ 6,  4,  4, 11,  3,  4, 12,  3,  4, 12,  2],\n",
      "        [ 5, 13, 13,  7,  3,  4,  7,  3,  4,  8,  2],\n",
      "        [ 5, 13, 11, 12,  3,  4, 13,  3,  4, 13,  2],\n",
      "        [ 5, 13, 13, 13,  3,  4, 10,  3,  4,  5,  2],\n",
      "        [ 6,  4,  6,  7,  3,  4,  5,  3,  4,  8,  2],\n",
      "        [ 5, 13, 11,  5,  3,  4,  7,  3,  4,  6,  2],\n",
      "        [ 5, 13, 11, 12,  3,  4, 11,  3,  4, 11,  2],\n",
      "        [ 6,  4,  4,  6,  3,  4,  9,  3,  4,  6,  2],\n",
      "        [ 6,  4,  6,  4,  3,  4, 11,  3,  4,  6,  2],\n",
      "        [ 5, 13, 13,  5,  3,  4, 12,  3,  4,  9,  2],\n",
      "        [ 6,  4,  5, 11,  3,  4,  5,  3,  4, 11,  2],\n",
      "        [ 5, 13, 12,  5,  3,  4, 10,  3,  4, 10,  2],\n",
      "        [ 5, 13, 12, 11,  3,  4, 13,  3,  4,  6,  2],\n",
      "        [ 5, 13, 11, 12,  3,  4,  7,  3,  4,  9,  2],\n",
      "        [ 6,  4,  4,  4,  3,  4, 13,  3,  4, 12,  2],\n",
      "        [ 6,  4,  5, 11,  3,  4,  7,  3,  4, 11,  2],\n",
      "        [ 5, 13, 12,  7,  3,  4, 11,  3,  4,  8,  2],\n",
      "        [ 6,  4,  4, 13,  3,  4, 11,  3,  4,  5,  2],\n",
      "        [ 5, 13, 13, 11,  3,  4, 11,  3,  4, 13,  2],\n",
      "        [ 5, 13, 13,  7,  3,  4, 11,  3,  4, 10,  2],\n",
      "        [ 5, 13, 12, 12,  3,  4, 12,  3,  4, 13,  2],\n",
      "        [ 6,  4,  4, 11,  3,  4,  8,  3,  4,  6,  2],\n",
      "        [ 6,  4,  5, 11,  3,  4, 10,  3,  4,  5,  2]])\n",
      "EncoderGRU(\n",
      "  (gru): GRU(37, 64, batch_first=True)\n",
      ")\n",
      "torch.Size([64, 6, 64]) torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x_enc_batch,  x_dec_batch, y_batch = next(iter(train_loader))\n",
    "print(x_enc_batch)\n",
    "print(x_dec_batch)\n",
    "print(y_batch)\n",
    "encoder = EncoderGRU(len(human_vocab), hidden_size=64, num_layers=1, bidirectional=False)\n",
    "print(encoder)\n",
    "enc_out, enc_hidden = encoder(x_enc_batch)\n",
    "print(enc_out.shape, enc_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8/71\n",
      "3/3/81\n",
      "4/8/96\n",
      "5/5/97\n",
      "9/7/98\n",
      "4/4/99\n",
      "2/8/02\n",
      "9/1/13\n",
      "6/2/75\n",
      "1/1/10\n",
      "1/8/21\n",
      "1/6/72\n",
      "8/8/92\n",
      "4/2/05\n",
      "5/4/71\n",
      "4/8/96\n",
      "6/2/23\n",
      "4/6/86\n",
      "9/1/98\n",
      "6/6/92\n",
      "3/1/86\n",
      "4/1/06\n",
      "9/4/73\n",
      "2/6/22\n",
      "3/5/12\n",
      "6/1/12\n",
      "1/4/98\n",
      "9/3/13\n",
      "1/7/98\n",
      "2/8/74\n",
      "2/6/93\n",
      "8/4/92\n",
      "2/2/72\n",
      "4/3/84\n",
      "8/8/89\n",
      "6/1/70\n",
      "2/6/77\n",
      "7/4/86\n",
      "9/9/20\n",
      "6/9/96\n",
      "2/1/22\n",
      "8/8/07\n",
      "3/4/93\n",
      "9/9/78\n",
      "6/1/99\n",
      "1/4/23\n",
      "3/2/71\n",
      "7/7/78\n",
      "5/2/02\n",
      "7/2/20\n",
      "8/5/91\n",
      "1/7/17\n",
      "6/6/81\n",
      "9/2/87\n",
      "3/5/78\n",
      "9/8/00\n",
      "3/7/17\n",
      "7/4/83\n",
      "7/1/09\n",
      "7/9/97\n",
      "7/6/93\n",
      "8/9/88\n",
      "4/2/07\n",
      "6/1/17\n"
     ]
    }
   ],
   "source": [
    "lang = train_dataset.input_lang\n",
    "for row in x_enc_batch:\n",
    "    print(lang.ind_to_str(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(machine_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderGRU(\n",
      "  (gru): GRU(14, 64, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=14, bias=True)\n",
      ")\n",
      "EncoderGRU(\n",
      "  (gru): GRU(37, 64, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderGRU(len(human_vocab), hidden_size=64, num_layers=1, bidirectional=False)\n",
    "decoder = DecoderGRU(len(machine_vocab), hidden_size=64, num_layers=1)\n",
    "print(decoder)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder forward pass\n",
      "\n",
      "Training with teacher forcing\n",
      "Input batch shape: torch.Size([64, 11])\n",
      "decoder output shape: torch.Size([64, 11, 14])\n",
      "decoder hn shape: torch.Size([1, 64, 64])\n",
      "\n",
      "Training without teacher forcing\n",
      "Decoder 1st input shape: torch.Size([64, 1])\n",
      "decoder output shape: torch.Size([64, 1, 14])\n",
      "decoder hn shape: torch.Size([1, 64, 64])\n",
      "decoder 2nd input shape: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoder forward pass\\n\")\n",
    "\n",
    "# Teacher forcing\n",
    "print(\"Training with teacher forcing\")\n",
    "print(f\"Input batch shape: {x_dec_batch.shape}\")\n",
    "dec_out, dec_hid = decoder(x_dec_batch, enc_hidden)\n",
    "print(f\"decoder output shape: {dec_out.shape}\\ndecoder hn shape: {dec_hid.shape}\")\n",
    "# loss(dec_out, target)\n",
    "print()\n",
    "# Without teacher forcing\n",
    "print(\"Training without teacher forcing\")\n",
    "dec_input = x_dec_batch[:,0:1]\n",
    "print(f\"Decoder 1st input shape: {dec_input.shape}\")\n",
    "dec_out, dec_hid = decoder(dec_input, enc_hidden)\n",
    "print(f\"decoder output shape: {dec_out.shape}\\ndecoder hn shape: {dec_hid.shape}\")\n",
    "next_input = torch.argmax(dec_out, dim=-1)\n",
    "print(f\"decoder 2nd input shape: {next_input.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2sec(\n",
       "  (encoder): EncoderGRU(\n",
       "    (gru): GRU(37, 64, batch_first=True)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (gru): GRU(14, 64, batch_first=True)\n",
       "    (fc): Linear(in_features=64, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2sec(encoder, decoder)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model forward pass (input -> encoder -> decoder -> output)\n",
      "\n",
      "Training without teacher forcing (out_length = 20)\n",
      "Input batch shape: torch.Size([64, 6])\n",
      "Output shape: torch.Size([64, 20, 14])\n",
      "\n",
      "Training with teacher forcing\n",
      "Encoder input batch shape: torch.Size([64, 6])\n",
      "Decoder input batch shape: torch.Size([64, 11])\n",
      "Output shape: torch.Size([64, 11, 14])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model forward pass (input -> encoder -> decoder -> output)\\n\")\n",
    "\n",
    "# Teacher forcing\n",
    "print(\"Training without teacher forcing (out_length = 20)\")\n",
    "print(f\"Input batch shape: {x_enc_batch.shape}\")\n",
    "output =  model(x_enc_batch, out_length = 20)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "print()\n",
    "# Without teacher forcing\n",
    "print(\"Training with teacher forcing\")\n",
    "print(f\"Encoder input batch shape: {x_enc_batch.shape}\")\n",
    "print(f\"Decoder input batch shape: {x_dec_batch.shape}\")\n",
    "output = model(x_enc_batch, dec_input_batch = x_dec_batch, teacher_forcing = True)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "trainer = Trainer(model, train_dataLoader=train_loader, val_dataLoader=val_loader, loss_fn= cross_entropy_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1    training loss: 0.428725 | validation loss: 0.480993\n",
      "Epoch  2    training loss: 0.413526 | validation loss: 0.463851\n",
      "Epoch  3    training loss: 0.398315 | validation loss: 0.444159\n",
      "Epoch  4    training loss: 0.380441 | validation loss: 0.427556\n",
      "Epoch  5    training loss: 0.362214 | validation loss: 0.413763\n",
      "Epoch  6    training loss: 0.346169 | validation loss: 0.403205\n",
      "Epoch  7    training loss: 0.330280 | validation loss: 0.379970\n",
      "Epoch  8    training loss: 0.311869 | validation loss: 0.361615\n",
      "Epoch  9    training loss: 0.296308 | validation loss: 0.351179\n",
      "Epoch  10   training loss: 0.281257 | validation loss: 0.345222\n",
      "Epoch  11   training loss: 0.271490 | validation loss: 0.328777\n",
      "Epoch  12   training loss: 0.255192 | validation loss: 0.306013\n",
      "Epoch  13   training loss: 0.241457 | validation loss: 0.293268\n",
      "Epoch  14   training loss: 0.234754 | validation loss: 0.291790\n",
      "Epoch  15   training loss: 0.226277 | validation loss: 0.288448\n",
      "Epoch  16   training loss: 0.216527 | validation loss: 0.276398\n",
      "Epoch  17   training loss: 0.203049 | validation loss: 0.262873\n",
      "Epoch  18   training loss: 0.196070 | validation loss: 0.246830\n",
      "Epoch  19   training loss: 0.185439 | validation loss: 0.240740\n",
      "Epoch  20   training loss: 0.184368 | validation loss: 0.235153\n",
      "Epoch  21   training loss: 0.174537 | validation loss: 0.221833\n",
      "Epoch  22   training loss: 0.164396 | validation loss: 0.201942\n",
      "Epoch  23   training loss: 0.151060 | validation loss: 0.186857\n",
      "Epoch  24   training loss: 0.140405 | validation loss: 0.178537\n",
      "Epoch  25   training loss: 0.130462 | validation loss: 0.168784\n",
      "Epoch  26   training loss: 0.130598 | validation loss: 0.159516\n",
      "Epoch  27   training loss: 0.120416 | validation loss: 0.151641\n",
      "Epoch  28   training loss: 0.112282 | validation loss: 0.148024\n",
      "Epoch  29   training loss: 0.108907 | validation loss: 0.140334\n",
      "Epoch  30   training loss: 0.106410 | validation loss: 0.126330\n",
      "Epoch  31   training loss: 0.098883 | validation loss: 0.130736\n",
      "Epoch  32   training loss: 0.096802 | validation loss: 0.127434\n",
      "Epoch  33   training loss: 0.092608 | validation loss: 0.129370\n",
      "Epoch  34   training loss: 0.086626 | validation loss: 0.112226\n",
      "Epoch  35   training loss: 0.079990 | validation loss: 0.108438\n",
      "Epoch  36   training loss: 0.073413 | validation loss: 0.104329\n",
      "Epoch  37   training loss: 0.068033 | validation loss: 0.102954\n",
      "Epoch  38   training loss: 0.063472 | validation loss: 0.100454\n",
      "Epoch  39   training loss: 0.060907 | validation loss: 0.094512\n",
      "Epoch  40   training loss: 0.060912 | validation loss: 0.094008\n",
      "Epoch  41   training loss: 0.058625 | validation loss: 0.090822\n",
      "Epoch  42   training loss: 0.058053 | validation loss: 0.090103\n",
      "Epoch  43   training loss: 0.056473 | validation loss: 0.086680\n",
      "Epoch  44   training loss: 0.053975 | validation loss: 0.087579\n",
      "Epoch  45   training loss: 0.052911 | validation loss: 0.082221\n",
      "Epoch  46   training loss: 0.053163 | validation loss: 0.078038\n",
      "Epoch  47   training loss: 0.056248 | validation loss: 0.098439\n",
      "Epoch  48   training loss: 0.056313 | validation loss: 0.089689\n",
      "Epoch  49   training loss: 0.061601 | validation loss: 0.078707\n",
      "Epoch  50   training loss: 0.067039 | validation loss: 0.088593\n"
     ]
    }
   ],
   "source": [
    "trainer.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translations(model, n = 10):\n",
    "    test_loader = DataLoader(dataset=val_dataset, collate_fn=collate_batch, batch_size = 64, num_workers = 8, shuffle=True)\n",
    "\n",
    "    x, z, y = next(iter(test_loader))\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    z = z.to(device)\n",
    "\n",
    "    y_hat = model(\n",
    "            x,\n",
    "            teacher_forcing=False,\n",
    "            sos_index=1,\n",
    "            out_length=11,\n",
    "        )\n",
    "\n",
    "    input_lang = train_dataset.input_lang\n",
    "    output_lang = train_dataset.target_lang\n",
    "\n",
    "    y_hat = y_hat.argmax(axis=-1)\n",
    "\n",
    "    print(\"Translation test\\n\")\n",
    "    print(\"Input                 |   Machine translation | Correct translation\")\n",
    "    for i in range(10):\n",
    "        print(input_lang.ind_to_str(x[i]).replace(';', ' '), output_lang.ind_to_str(y_hat[i]).strip('<>;'), \" \"*10, output_lang.ind_to_str(y[i]).strip(';<>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation test\n",
      "\n",
      "Input                 |   Machine translation | Correct translation\n",
      "22 december 1999            1997-12-22            1999-12-22\n",
      "wednesday september 9 2020  2020-09-09            2020-09-09\n",
      "monday november 29 1982     1982-11-29            1982-11-29\n",
      "april 18 2013               2029-04-18            2013-04-18\n",
      "may 14 2001                 2028-05-14            2001-05-14\n",
      "3 nov 1995                  2006-11-03            1995-11-03\n",
      "february 18 1981            2010-02-17            1981-02-18\n",
      "tuesday june 13 1978        1978-06-13            1978-06-13\n",
      "friday june 19 1992         2010-07-19            1992-06-19\n",
      "11 april 1978               2018-04-11            1978-04-11\n"
     ]
    }
   ],
   "source": [
    "print_translations(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        print(scores.shape)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 14])\n",
      "torch.Size([5, 1, 32]) torch.Size([5, 1, 14])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "output_size = len(machine_vocab)\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "att = BahdanauAttention(hidden_size)\n",
    "\n",
    "hidden = torch.zeros(1, batch_size, hidden_size)\n",
    "keys = torch.zeros(batch_size, output_size, hidden_size) # encoder outputs\n",
    "\n",
    "query = hidden.permute(1, 0, 2)\n",
    "\n",
    "c, w = att(query, keys)\n",
    "print(c.shape, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(AttentionDecoderGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, sos_index = 1, decoder_input = None, teacher_forcing = False, out_length = 1):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(sos_index)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(out_length):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if teacher_forcing:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = decoder_input[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        # input_gru = torch.cat((embedded, context), dim=2)\n",
    "        input_gru = context\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2sec(nn.Module):\n",
    "    def __init__(self, encoder: EncoderGRU, decoder: AttentionDecoderGRU) -> None:\n",
    "        super(Seq2sec, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_input_batch, sos_index = 1, decoder_input = None, teacher_forcing = False, out_length = 1):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(enc_input_batch)\n",
    "        batch_size = len(enc_input_batch)\n",
    "        \n",
    "        decoder_output, _, _ = decoder(encoder_outputs, encoder_hidden, sos_index, decoder_input, teacher_forcing, out_length)\n",
    "        \n",
    "        return decoder_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
